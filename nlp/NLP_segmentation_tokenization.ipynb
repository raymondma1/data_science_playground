{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence and Word Segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell for a HINT:\n",
    "import base64\n",
    "base64.decodestring('S2VlcCBhIGxpc3Qgb2Ygc2VudGVuY2VzLCBhbmQgYSB0ZW1wIHN0cmluZyB3aXRoIHRoZSBjdXJy\\nZW50IHNlbnRlbmNlLiBBcHBlbmQgd2hlbiB5b3UgaGl0IHRoZSByaWdodCBjaGFyYWN0ZXJz\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the input text and what the output should be.\n",
    "\n",
    "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
    "easy_split_text = [\"I went to the zoo today.\",\n",
    "                   \"What do you think of that?\",\n",
    "                   \"I bet you hate it!\",\n",
    "                   \"Or maybe you don't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function to split a string into sentences.\n",
    "\n",
    "def sentencer(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
    "    \n",
    "    sentences = []\n",
    "    substring = ''\n",
    "    for c in text:\n",
    "        if c in ('.', '!', '?'):\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        else:\n",
    "            substring += c\n",
    "\n",
    "#     if substring:\n",
    "#         sentences.append(substring)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation continued\n",
    "\n",
    "accommodate 'Dr.', 'Mrs.', 'Mr.', and 'Ms.'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the input text and what the output should be.\n",
    "\n",
    "med_text = \"My name is Dr. Lee. There is also a Mrs. Lee. Actually, there are tons! They're other people's wives.\"\n",
    "med_split_text = [\"My name is Dr. Lee.\",\n",
    "                  \"There is also a Mrs. Lee.\",\n",
    "                  \"Actually, there are tons!\",\n",
    "                  \"They're other people's wives.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modified sentencer to account for these new patterns.\n",
    "\n",
    "\n",
    "def sentencer2(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
    "    # FILL IN CODE\n",
    "\n",
    "    sentences = []\n",
    "    substring = ''\n",
    "    for c in text:\n",
    "        if c in ('!', '?'):\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        elif c == '.':\n",
    "            if substring[-3:].lower() == 'mrs' or substring[-2:].lower() in ['dr', 'mr', 'ms']:\n",
    "                substring += c\n",
    "            else:\n",
    "                sentences.append(substring + c)\n",
    "                substring = ''\n",
    "        else:\n",
    "            substring += c\n",
    "\n",
    "    if substring:\n",
    "        sentences.append(substring)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation continued\n",
    "\n",
    "Abbreviations like 'a.k.a.' are harder to accommodate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the input text and what the output should be.\n",
    "\n",
    "hard_text = \"I know an M.D., i.e. a doctor. Like Dr. Smith, a.k.a. Docsmith.\"\n",
    "hard_split_text = [\"I know an M.D., i.e. a doctor.\",\n",
    "                   \"Like Dr. Smith, a.k.a. Docsmith.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modify your last sentencer to account for these new patterns.\n",
    "\n",
    "def sentencer3(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a sentence'''\n",
    "    # FILL IN CODE\n",
    "\n",
    "    sentences = []\n",
    "    substring = ''\n",
    "    attach = False\n",
    "    for c in text:\n",
    "        if c in ('!', '?'):\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        elif c == '.':\n",
    "            if len(substring) == 1:\n",
    "                sentences[-1] += substring + c\n",
    "                substring = ''\n",
    "                attach = True\n",
    "            elif substring[-3:].lower() == 'mrs' or substring[-2:].lower() in ['dr', 'mr', 'ms']:\n",
    "                substring += c\n",
    "            else:\n",
    "                if attach:\n",
    "                    sentences[-1] += substring + c\n",
    "                    substring = ''\n",
    "                    attach = False\n",
    "                else:\n",
    "                    sentences.append(substring + c)\n",
    "                    substring = ''\n",
    "        else:\n",
    "            substring += c\n",
    "\n",
    "    if substring:\n",
    "        sentences.append(substring)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if map(str.strip, sentencer3(hard_text)) == hard_split_text:\n",
    "    print 'Passed!'\n",
    "else:\n",
    "    print 'Sorry, try again!'\n",
    "    print\n",
    "    print 'Actual output:'\n",
    "    print sentencer3(hard_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print hard_split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation continued\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# downloading the Punkt Tokenizer Models.\n",
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sent_detector.sentences_from_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I went to the zoo today.', 'What do you think of that?', 'I bet you hate it!', \"Or maybe you don't\"]\n",
      "['My name is Dr. Lee.', 'There is also a Mrs. Lee.', 'Actually, there are tons!', \"They're other people's wives.\"]\n",
      "['I know an M.D., i.e.', 'a doctor.', 'Like Dr. Smith, a.k.a.', 'Docsmith.']\n"
     ]
    }
   ],
   "source": [
    "print sent_detector.sentences_from_text(easy_text)\n",
    "print sent_detector.sentences_from_text(med_text)\n",
    "print sent_detector.sentences_from_text(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define our objective tokenizations. Note that we've removed some punctuation.\n",
    "easy_words = ['I', 'went', 'to', 'the', 'zoo', 'today',\n",
    "              'What', 'do', 'you', 'think', 'of', 'that',\n",
    "              'I', 'bet', 'you', 'hate', 'it',\n",
    "              'Or', 'maybe', 'you', \"don't\"]\n",
    "med_words = ['My', 'name', 'is', 'Dr', 'Lee',\n",
    "             'There', 'is', 'also', 'a', 'Mrs', 'Lee',\n",
    "             'Actually,', 'there', 'are', 'tons',\n",
    "             \"They're\", 'other', \"people's\", 'wives']\n",
    "hard_words = ['I', 'know', 'an', 'MD,', 'ie', 'a', 'doctor',\n",
    "              'Like', 'Dr', 'Smith,', 'aka', 'Docsmith']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function to split a string into words.\n",
    "\n",
    "def tokenizer(text):\n",
    "    '''take a string called `text` and return a list of strings, each containing a WORD'''\n",
    "    cleaned_text = text.replace('.', '').replace('?', '').replace('!', '')\n",
    "    return cleaned_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if tokenizer(easy_text) == easy_words:\n",
    "    print 'Passed!'\n",
    "else:\n",
    "    print 'Failed!'\n",
    "    print\n",
    "    print 'Actual Output:'\n",
    "    print tokenizer(easy_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print easy_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if tokenizer(med_text) == med_words:\n",
    "    print 'Passed!'\n",
    "else:\n",
    "    print 'Failed!'\n",
    "    print\n",
    "    print 'Actual Output:'\n",
    "    print tokenizer(med_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print med_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
   "source": [
    "# test your function by running this cell\n",
    "\n",
    "if tokenizer(hard_text) == hard_words:\n",
    "    print 'Congratulations!'\n",
    "else:\n",
    "    print 'Failed!'\n",
    "    print\n",
    "    print 'Actual output:'\n",
    "    print tokenizer(hard_text)\n",
    "    print\n",
    "    print 'Desired output:'\n",
    "    print hard_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'know',\n",
       " 'an',\n",
       " 'MD,',\n",
       " 'ie',\n",
       " 'a',\n",
       " 'doctor',\n",
       " 'Like',\n",
       " 'Dr',\n",
       " 'Smith,',\n",
       " 'aka',\n",
       " 'Docsmith']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization continued\n",
    "\n",
    "Let's see how NLTK [tokenizes text into words](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'went', 'to', 'the', 'zoo', 'today', '.', 'What', 'do', 'you', 'think', 'of', 'that', '?', 'I', 'bet', 'you', 'hate', 'it', '!', 'Or', 'maybe', 'you', 'do', \"n't\"]\n",
      "['My', 'name', 'is', 'Dr.', 'Lee', '.', 'There', 'is', 'also', 'a', 'Mrs.', 'Lee', '.', 'Actually', ',', 'there', 'are', 'tons', '!', 'They', \"'re\", 'other', 'people', \"'s\", 'wives', '.']\n",
      "['I', 'know', 'an', 'M.D.', ',', 'i.e', '.', 'a', 'doctor', '.', 'Like', 'Dr.', 'Smith', ',', 'a.k.a', '.', 'Docsmith', '.']\n"
     ]
    }
   ],
   "source": [
    "print word_tokenize(easy_text)\n",
    "print word_tokenize(med_text)\n",
    "print word_tokenize(hard_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a version based on pattern-matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'went', 'to', 'the', 'zoo', 'today', '.', 'What', 'do', 'you', 'think', 'of', 'that', '?', 'I', 'bet', 'you', 'hate', 'it', '!', 'Or', 'maybe', 'you', 'don', \"'\", 't']\n",
      "['My', 'name', 'is', 'Dr', '.', 'Lee', '.', 'There', 'is', 'also', 'a', 'Mrs', '.', 'Lee', '.', 'Actually', ',', 'there', 'are', 'tons', '!', 'They', \"'\", 're', 'other', 'people', \"'\", 's', 'wives', '.']\n",
      "['I', 'know', 'an', 'M', '.', 'D', '.,', 'i', '.', 'e', '.', 'a', 'doctor', '.', 'Like', 'Dr', '.', 'Smith', ',', 'a', '.', 'k', '.', 'a', '.', 'Docsmith', '.']\n"
     ]
    }
   ],
   "source": [
    "print wordpunct_tokenize(easy_text)\n",
    "print wordpunct_tokenize(med_text)\n",
    "print wordpunct_tokenize(hard_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'went', 'to')\n",
      "('went', 'to', 'the')\n",
      "('to', 'the', 'zoo')\n",
      "('the', 'zoo', 'today')\n",
      "('zoo', 'today', '.')\n",
      "('today', '.', 'What')\n",
      "('.', 'What', 'do')\n",
      "('What', 'do', 'you')\n",
      "('do', 'you', 'think')\n",
      "('you', 'think', 'of')\n",
      "('think', 'of', 'that')\n",
      "('of', 'that', '?')\n",
      "('that', '?', 'I')\n",
      "('?', 'I', 'bet')\n",
      "('I', 'bet', 'you')\n",
      "('bet', 'you', 'hate')\n",
      "('you', 'hate', 'it')\n",
      "('hate', 'it', '!')\n",
      "('it', '!', 'Or')\n",
      "('!', 'Or', 'maybe')\n",
      "('Or', 'maybe', 'you')\n",
      "('maybe', 'you', 'do')\n",
      "('you', 'do', \"n't\")\n"
     ]
    }
   ],
   "source": [
    "for bigram in trigrams(word_tokenize(easy_text)):\n",
    "    print bigram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
